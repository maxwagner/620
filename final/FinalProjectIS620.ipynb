{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS620 Final Project\n",
    "\n",
    "Data:  ~4200 pub med journals, including author affilations.  The affiliations are not provided in an specified manner, and are instead long 'strings'.  \n",
    "\n",
    "Goal:  It is not clear, looking at the strings, how to distinuish instituion names from department names and so on.  The goal here is to develop a maching learning algo which can pull instinution names from the affilations, such that a graph of schools which publish with eachother might be generated.\n",
    "\n",
    "Results:  A decision tree was found to be most appliciable.  The accuracy was ~92% with the test set.  However, this only tells part of the story; I labeled a training set of ~4000 samples, and some of these I mislabeled.  Thus the machine learning had to properly assign labels that I had put in incorrectly!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here we open a file of downloaded journal articles including author names and affiliations.\n",
    "pubs = []\n",
    "with open('project2pubs.txt','r') as f:\n",
    "    for line in f:\n",
    "        pubs.append(line.split(']]'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'], ['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'], ['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'], ['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'], ['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'], ['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X'\",\n",
       " ',Key Laboratory of Stem Cell Biology, Institute of Health Sciences, Shanghai Institutes for Biological Sciences &Shanghai Jiao Tong University School of Medicine (SJTUSM), Chinese Academy of Sciences, Shanghai, China. Key Laboratory of Stem Cell Biology, Institute of Health Sciences, Shanghai Institutes for Biological Sciences &Shanghai Jiao Tong University School of Medicine (SJTUSM), Chinese Academy of Sciences, Shanghai, China. Key Laboratory of Stem Cell Biology, Institute of Health Sciences, Shanghai Institutes for Biological Sciences &Shanghai Jiao Tong University School of Medicine (SJTUSM), Chinese Academy of Sciences, Shanghai, China. Key Laboratory of Stem Cell Biology, Institute of Health Sciences, Shanghai Institutes for Biological Sciences &Shanghai Jiao Tong University School of Medicine (SJTUSM), Chinese Academy of Sciences, Shanghai, China. Department of Cellular and Molecular Medicine, University of California at San Diego, La Jolla, La Jolla, California, USA. Key Laboratory of Stem Cell Biology, Institute of Health Sciences, Shanghai Institutes for Biological Sciences &Shanghai Jiao Tong University School of Medicine (SJTUSM), Chinese Academy of Sciences, Shanghai, China. Collaborative Innovation Center of Systems Biomedicine, Shanghai Jiao Tong University School of Medicine, Shanghai, China.,2016/10/11 06:00,Nat Methods,Nature methods\\n']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## As you can see, the data is a mess\n",
    "pubs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ma Y', 'Zhang J', 'Yin W', 'Zhang Z', 'Song Y', 'Chang X']"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The form of the strings in the files, are a mess, a much effort needed to clean it all up\n",
    "stringIn = \"string.with.punctuation!\"\n",
    "\n",
    "author = []\n",
    "for x in pubs:\n",
    "    if isinstance(x, list):\n",
    "        t = (x[0].translate(None, '!@#$][,').split(\"'\"))\n",
    "        temp = []\n",
    "        for x in t: \n",
    "            if x:\n",
    "                if x != \" \":\n",
    "                    if x not in temp:\n",
    "                       temp.append(x)\n",
    "        author.append(temp)\n",
    "            \n",
    "        \n",
    "    \n",
    "author[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4215"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4215"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The affiliation data is a mess.  Sometimes its deliminated with . or ;, etcetera.  \n",
    "## below we split up each affiliation using a number of delimiters\n",
    "\n",
    "splitcomma = []\n",
    "splitperiod = []\n",
    "splitsemicolon = []\n",
    "splitspace = []\n",
    "for x in pubs:\n",
    "    if len(x) > 1:\n",
    "        s = x[1]\n",
    "        s = s[:-1]\n",
    "        t = s.split(\",\")\n",
    "        if len(t)>2:\n",
    "            t = t[:-2]  # remove journal names\n",
    "        else:\n",
    "            t = \" \"\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitcomma.append(t)\n",
    "        t = s.split(\".\")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitperiod.append(t)\n",
    "        t = s.split(\";\")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitsemicolon.append(t)\n",
    "        t = s.split(\" \")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitspace.append(t)\n",
    "    else:\n",
    "        splitcomma.append(\"\")\n",
    "        splitperiod.append(\"\")\n",
    "        splitsemicolon.append(\"\")\n",
    "        splitspace.append(\"\")\n",
    "        \n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringHasPunct(s):\n",
    "    if not s:\n",
    "        return True\n",
    "    for x in string.punctuation:\n",
    "        if x in s:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## below we generate a list of all possible 'phrases' in the affiliation\n",
    "\n",
    "listterms = []\n",
    "for x in range(0, len(splitcomma)):\n",
    "    temp = []\n",
    "    t = []\n",
    "    for y in splitcomma[x]:\n",
    "        if not stringHasPunct(y):\n",
    "            t.append(y)\n",
    "    temp.append(t)\n",
    "    t = []\n",
    "    for y in splitsemicolon[x]:\n",
    "        if not stringHasPunct(y):\n",
    "            t.append(y)\n",
    "    temp.append(t)\n",
    "    t = []\n",
    "    for y in splitperiod[x]:\n",
    "        if not stringHasPunct(y):\n",
    "            t.append(y)\n",
    "    temp.append(t)\n",
    "    listterms.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Qinghai Provincial Key Laboratory for Plague Control and Research',\n",
       "  'Qinghai Institute for Endemic Disease Prevention and Control',\n",
       "  'Xining',\n",
       "  'Qinghai Province',\n",
       "  '811602',\n",
       "  'Fengtai',\n",
       "  'Beijing',\n",
       "  '100071',\n",
       "  'Qinghai Institute for Endemic Disease Prevention and Control',\n",
       "  'Xining',\n",
       "  'Qinghai Province',\n",
       "  '811602',\n",
       "  'Fengtai',\n",
       "  'Beijing',\n",
       "  '100071'],\n",
       " [],\n",
       " ['com']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example affiliations are shown below.  The question is, can\n",
    "## a machine learning classification determine which are institution names??\n",
    "listterms[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I save all the phrases to a csv.  I will label some of the \n",
    "## phrases in the csv file to then train the system.\n",
    "f = open('labelAffiliations.txt','w')\n",
    "for x in listterms:\n",
    "    for y in x:\n",
    "        for z in y:\n",
    "            f.write(str(z) + '\\n') \n",
    "f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## in excel, I marked >4000 'phrases' as belonging to an \n",
    "## instituion or not.  Most of these were just zip codes and such,\n",
    "## so easy to mark.\n",
    "\n",
    "phrase = []\n",
    "label = []\n",
    "with open('labelAffiliations.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        line = line.split(\",\")\n",
    "        if len(line) >1 and line[1]:\n",
    "            phrase.append(line[0])\n",
    "            label.append(line[1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4866"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get all words used in any labeled affiliation\n",
    "words = []\n",
    "for x in phrase:\n",
    "    x = x.split(\" \")\n",
    "    for y in x:\n",
    "        words.append(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15304"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at the top 200 words\n",
    "all_words = nltk.FreqDist(words)\n",
    "top100 = all_words.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_feature = []\n",
    "for x in top100:\n",
    "    word_feature.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'and',\n",
       " 'Department',\n",
       " 'for',\n",
       " 'Center',\n",
       " 'Biology',\n",
       " 'University',\n",
       " 'Institute',\n",
       " 'Research',\n",
       " 'Sciences',\n",
       " 'Medicine',\n",
       " 'Division',\n",
       " 'Science',\n",
       " 'Molecular',\n",
       " 'Cell',\n",
       " 'Medical',\n",
       " 'Centre',\n",
       " 'College',\n",
       " 'Cancer',\n",
       " 'Street',\n",
       " 'de',\n",
       " 'Genetics',\n",
       " 'Laboratory',\n",
       " 'Road',\n",
       " 'Engineering',\n",
       " 'Avenue',\n",
       " 'National',\n",
       " 'in',\n",
       " 'Beijing',\n",
       " 'Microbiology',\n",
       " 'Technology',\n",
       " 'New',\n",
       " 'Health',\n",
       " 'Biological',\n",
       " 'Biotechnology',\n",
       " 'Drive',\n",
       " 'School',\n",
       " 'China',\n",
       " 'CA',\n",
       " 'Cambridge',\n",
       " 'Biochemistry',\n",
       " 'Biol',\n",
       " 'a',\n",
       " 'the',\n",
       " 'York',\n",
       " 'Life',\n",
       " 'Diseases',\n",
       " 'California',\n",
       " '1',\n",
       " 'Biomedical']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Key Laboratory of Stem Cell Biology',\n",
       "  'Institute of Health Sciences',\n",
       "  'Chinese Academy of Sciences',\n",
       "  'Shanghai',\n",
       "  'Institute of Health Sciences',\n",
       "  'Chinese Academy of Sciences',\n",
       "  'Shanghai',\n",
       "  'Institute of Health Sciences',\n",
       "  'Chinese Academy of Sciences',\n",
       "  'Shanghai',\n",
       "  'Institute of Health Sciences',\n",
       "  'Chinese Academy of Sciences',\n",
       "  'Shanghai',\n",
       "  'University of California at San Diego',\n",
       "  'La Jolla',\n",
       "  'La Jolla',\n",
       "  'California',\n",
       "  'Institute of Health Sciences',\n",
       "  'Chinese Academy of Sciences',\n",
       "  'Shanghai',\n",
       "  'Shanghai Jiao Tong University School of Medicine',\n",
       "  'Shanghai'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listterms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## the feature I want to look at.  \n",
    "\n",
    "def getContentIndex(listterms_x, phrase):\n",
    "    '''what affiliation does a phrase occur in.\n",
    "    Bear in mind that I labeled phrases free of any context\n",
    "    and now need to figure out which affilation each phrase come from'''\n",
    "    for x in listterms_x: ## x is a list ['ewfew', 'wefew' ]\n",
    "        if phrase in x:\n",
    "            return x.index(phrase), x\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "def hasNumber(word):\n",
    "    for x in word:\n",
    "        if is_number(x):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def firstWordisCapital(x):\n",
    "    fl = x[0]\n",
    "    if is_number(fl):\n",
    "        return False\n",
    "    if fl == fl.upper():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def ratioOfCaptialLetters(phrase):\n",
    "    p = phrase.split()\n",
    "    c = 0\n",
    "    for x in p:\n",
    "        for y in x:\n",
    "            if not is_number(y):\n",
    "                if y == y.upper():\n",
    "                    c += 1\n",
    "    if len(p) != 0:\n",
    "        r = c/float(len(p))\n",
    "    else:\n",
    "        r = 0\n",
    "    return int(r*10)\n",
    "\n",
    "def numberOfWords(phrase):\n",
    "    p = phrase.split()\n",
    "    return int(len(p)/2)\n",
    "\n",
    "def oneWord(phrase):\n",
    "    p = phrase.split()\n",
    "    if len(p) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def totalLen(phrase):\n",
    "    return int(len(phrase)/10)\n",
    "    \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in word_feature:\n",
    "        (word in set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features_old(context, phrase, word_feature):\n",
    "    doci, i = getContentIndex(context, phrase)\n",
    "    \n",
    "    context = context[i]\n",
    "    document_words = set(phrase.split())\n",
    "    features = {}\n",
    "    for word in word_feature:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    features['isnumeric'] = is_number(phrase)\n",
    "    features['hasANumberInIt'] = hasNumber(phrase)\n",
    "    features['firstLetterCapital'] = firstWordisCapital(phrase)\n",
    "    features['ratioOfCaptialLetters'] = ratioOfCaptialLetters(phrase)\n",
    "    features['numberOfWords'] = numberOfWords(phrase)\n",
    "    features['startsWithNumber'] = is_number(phrase[0])\n",
    "    features['totalLength'] = totalLen(phrase)\n",
    "    features['oneWord'] = oneWord(phrase)\n",
    "    \n",
    "    if doci != 0:\n",
    "        phrasebefore = context[doci-1]\n",
    "        document_words = set(phrasebefore)\n",
    "        #for word in word_feature:\n",
    "            #features['phraseBeforeContains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericBefore'] = is_number(phrasebefore)\n",
    "        features['hasANumberInItBefore'] = hasNumber(phrasebefore)\n",
    "        features['firstLetterCapitalBefore'] = firstWordisCapital(phrasebefore)\n",
    "        features['ratioOfCaptialLettersBefore'] = ratioOfCaptialLetters(phrasebefore)\n",
    "        features['numberOfWordsBefore'] = numberOfWords(phrasebefore)\n",
    "        features['startsWithNumberBefore'] = is_number(phrasebefore[0])\n",
    "        features['totalLengthBefore'] = totalLen(phrasebefore)\n",
    "        features['oneWordBefore'] = oneWord(phrasebefore)\n",
    "        \n",
    "    \n",
    "    if doci + 1 < len(context):\n",
    "        phraseafter = context[doci+1]\n",
    "        document_words = set(phraseafter)\n",
    "        #for word in word_feature:\n",
    "            #features['phraseAfterContains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericAfter'] = is_number(phraseafter)\n",
    "        features['hasANumberInItAfter'] = hasNumber(phraseafter)\n",
    "        features['firstLetterCapitalAfter'] = firstWordisCapital(phraseafter)\n",
    "        features['ratioOfCaptialLettersAfter'] = ratioOfCaptialLetters(phraseafter)\n",
    "        features['numberOfWordsAfter'] = numberOfWords(phraseafter)\n",
    "        features['startsWithNumberAfter'] = is_number(phraseafter[0])\n",
    "        features['totalLengthAfter'] = totalLen(phraseafter)\n",
    "        features['oneWordAfter'] = oneWord(phraseafter)\n",
    "    if doci + 2 < len(context):\n",
    "        phraseafter = context[doci+2]\n",
    "        document_words = set(phraseafter)\n",
    "        #for word in word_feature:\n",
    "            #features['phraseAfter2Contains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericAfter2'] = is_number(phraseafter)\n",
    "        features['hasANumberInItAfter2'] = hasNumber(phraseafter)\n",
    "        features['firstLetterCapitalAfter2'] = firstWordisCapital(phraseafter)\n",
    "        features['ratioOfCaptialLettersAfter2'] = ratioOfCaptialLetters(phraseafter)\n",
    "        features['numberOfWordsAfter2'] = numberOfWords(phrase)\n",
    "        features['totalLengthAfter2'] = totalLen(phraseafter)\n",
    "        features['oneWordAfter2'] = oneWord(phraseafter)\n",
    "    if doci + 3 < len(context):\n",
    "        phraseafter = context[doci+3]\n",
    "        document_words = set(phraseafter)\n",
    "        #for word in word_feature:\n",
    "            #features['phraseAfter3Contains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericAfter3'] = is_number(phraseafter)\n",
    "        features['hasANumberInItAfter3'] = hasNumber(phraseafter)\n",
    "        features['firstLetterCapitalAfter3'] = firstWordisCapital(phraseafter)\n",
    "        features['ratioOfCaptialLettersAfter3'] = ratioOfCaptialLetters(phraseafter)\n",
    "        features['numberOfWordsAfter3'] = numberOfWords(phraseafter)\n",
    "        features['totalLengthAfter3'] = totalLen(phraseafter)\n",
    "        \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(context, phrase, word_feature):\n",
    "    ## context is the 'listterms' which is a list of 3 lists\n",
    "\n",
    "    doci, context = getContentIndex(context, phrase)\n",
    "    document_words = set(phrase.split())\n",
    "    features = {}\n",
    "    for word in word_feature:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    features['isnumeric'] = is_number(phrase)\n",
    "    features['hasANumberInIt'] = hasNumber(phrase)\n",
    "    features['firstLetterCapital'] = firstWordisCapital(phrase)\n",
    "    features['ratioOfCaptialLetters'] = ratioOfCaptialLetters(phrase)\n",
    "    features['numberOfWords'] = numberOfWords(phrase)\n",
    "    features['startsWithNumber'] = is_number(phrase[0])\n",
    "    features['totalLength'] = totalLen(phrase)\n",
    "    features['oneWord'] = oneWord(phrase)\n",
    "    \n",
    "    if doci != 0:\n",
    "        phrasebefore = context[doci-1]\n",
    "        document_words = set(phrasebefore.split())\n",
    "        #for word in word_feature:\n",
    "            #features['phraseBeforeContains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericBefore'] = is_number(phrasebefore)\n",
    "        features['hasANumberInItBefore'] = hasNumber(phrasebefore)\n",
    "        features['firstLetterCapitalBefore'] = firstWordisCapital(phrasebefore)\n",
    "        features['ratioOfCaptialLettersBefore'] = ratioOfCaptialLetters(phrasebefore)\n",
    "        features['numberOfWordsBefore'] = numberOfWords(phrasebefore)\n",
    "        features['startsWithNumberBefore'] = is_number(phrasebefore[0])\n",
    "        features['totalLengthBefore'] = totalLen(phrasebefore)\n",
    "        features['oneWordBefore'] = oneWord(phrasebefore)\n",
    "    if doci + 1 < len(context):\n",
    "        phraseafter = context[doci+1]\n",
    "        document_words = set(phraseafter.split())\n",
    "        #for word in word_feature:\n",
    "            #features['phraseAfterContains(%s)' % word] = (word in set(document_words))\n",
    "        features['isnumericAfter'] = is_number(phraseafter)\n",
    "        features['hasANumberInItAfter'] = hasNumber(phraseafter)\n",
    "        features['firstLetterCapitalAfter'] = firstWordisCapital(phraseafter)\n",
    "        features['ratioOfCaptialLettersAfter'] = ratioOfCaptialLetters(phraseafter)\n",
    "        features['numberOfWordsAfter'] = numberOfWords(phraseafter)\n",
    "        features['startsWithNumberAfter'] = is_number(phraseafter[0])\n",
    "        features['totalLengthAfter'] = totalLen(phraseafter)\n",
    "        features['oneWordAfter'] = oneWord(phraseafter)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(context, phrase, word_feature):\n",
    "    ## context is the 'listterms' which is a list of 3 lists\n",
    "\n",
    "    document_words = set(phrase.split())\n",
    "    features = {}\n",
    "    for word in word_feature:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    features['isnumeric'] = is_number(phrase)\n",
    "    features['hasANumberInIt'] = hasNumber(phrase)\n",
    "    features['firstLetterCapital'] = firstWordisCapital(phrase)\n",
    "    features['ratioOfCaptialLetters'] = ratioOfCaptialLetters(phrase)\n",
    "    features['numberOfWords'] = numberOfWords(phrase)\n",
    "    features['startsWithNumber'] = is_number(phrase[0])\n",
    "    features['totalLength'] = totalLen(phrase)\n",
    "    features['oneWord'] = oneWord(phrase)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_phrases = {}\n",
    "for x in range(0, len(phrase)):\n",
    "    d_phrases[phrase[x]] = label[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4177"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(listterms)\n",
    "len(d_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Graduate School of Arts and Sciences',\n",
       "  'The University of Tokyo',\n",
       "  'The University of Tokyo',\n",
       "  'The University of Tokyo',\n",
       "  'The University of Tokyo',\n",
       "  'The University of Tokyo'],\n",
       " [],\n",
       " ['ecc', 'ac', 'jp']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listterms[0] ## each list has 3 lists (split by comma, semicolon, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "phrasesLookedUp = []\n",
    "for x in listterms:\n",
    "    for y in x:\n",
    "        for z in y:\n",
    "            if z in d_phrases:\n",
    "                featuresets.append([document_features(x, z, word_feature), d_phrases[z]]) \n",
    "                phrasesLookedUp.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[2500:], featuresets[:2500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9528\n"
     ]
    }
   ],
   "source": [
    "## achieve 90% accuracy\n",
    "print nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 92% of the test set are not instituions.  At first this seems like bad news.\n",
    "## But look at errors below\n",
    "c = 0\n",
    "for x in test_set:\n",
    "    if x[1] == 's':\n",
    "        c += 1\n",
    "1-float(c)/len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   ratioOfCaptialLetters = 6                   g : s      =     76.9 : 1.0\n",
      "       contains(Biology) = True                g : s      =     62.5 : 1.0\n",
      "      contains(Sciences) = True                g : s      =     58.9 : 1.0\n",
      "    contains(Technology) = True                s : g      =     40.9 : 1.0\n",
      "    contains(University) = True                s : g      =     24.4 : 1.0\n",
      "           numberOfWords = 4                   g : s      =     21.6 : 1.0\n",
      "             totalLength = 6                   g : s      =     18.5 : 1.0\n",
      "           contains(for) = True                g : s      =     15.1 : 1.0\n",
      "        contains(School) = True                s : g      =     12.7 : 1.0\n",
      "        contains(Health) = True                s : g      =     12.5 : 1.0\n",
      "          contains(Life) = True                g : s      =     11.2 : 1.0\n",
      "      contains(Research) = True                g : s      =     10.6 : 1.0\n",
      "           numberOfWords = 5                   g : s      =      9.6 : 1.0\n",
      "   ratioOfCaptialLetters = 17                  s : g      =      9.1 : 1.0\n",
      "      contains(National) = True                s : g      =      8.8 : 1.0\n",
      "        contains(Centre) = True                g : s      =      8.4 : 1.0\n",
      "           contains(and) = True                g : s      =      8.1 : 1.0\n",
      "       contains(Science) = True                g : s      =      7.6 : 1.0\n",
      "     contains(Institute) = True                s : g      =      7.5 : 1.0\n",
      "     contains(Molecular) = True                g : s      =      6.0 : 1.0\n",
      "             totalLength = 7                   g : s      =      5.3 : 1.0\n",
      "             totalLength = 3                   s : g      =      5.2 : 1.0\n",
      "    contains(California) = True                g : s      =      4.6 : 1.0\n",
      "       contains(College) = True                g : s      =      4.4 : 1.0\n",
      "       contains(Medical) = True                s : g      =      3.7 : 1.0\n",
      "         contains(China) = True                s : g      =      3.6 : 1.0\n",
      "             totalLength = 4                   g : s      =      3.3 : 1.0\n",
      "   ratioOfCaptialLetters = 7                   s : g      =      2.7 : 1.0\n",
      "           numberOfWords = 2                   s : g      =      2.7 : 1.0\n",
      "           contains(New) = True                g : s      =      2.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "i = 0\n",
    "for (name, tag) in test_set:\n",
    "    guess = classifier.classify(name)\n",
    "    phrase1 = phrasesLookedUp[i]\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, phrase1) )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 's', 'David H'),\n",
       " ('g', 's', 'Broad Institute of MIT and Harvard'),\n",
       " ('g', 's', 'Academy of Military Medical Sciences'),\n",
       " ('g', 's', 'Beijing University of Technology'),\n",
       " ('g', 's', 'Campus Giessen'),\n",
       " ('g', 's', 'Atlanta'),\n",
       " ('s', 'g', 'David Geffen School of Medicine'),\n",
       " ('g', 's', 'College of Veterinary Medicine'),\n",
       " ('s', 'g', 'Massachusetts General Hospital'),\n",
       " ('g', 's', 'Berlin'),\n",
       " ('g', 's', 'Chinese Academy of Medical Sciences'),\n",
       " ('g', 's', 'Birmingham VA Medical Center'),\n",
       " ('g', 's', 'Anhui University'),\n",
       " ('g', 's', 'Boston'),\n",
       " ('g', 's', 'Columbus'),\n",
       " ('g', 's', 'Athens'),\n",
       " ('g', 's', 'Danish Technical University'),\n",
       " ('g', 's', 'Brisbane'),\n",
       " ('g', 's', 'China CUHK Shenzhen Research Institute'),\n",
       " ('g', 's', 'Atlanta VA Medical Center'),\n",
       " ('g', 's', 'Bloomington'),\n",
       " ('g', 's', 'Churchill Hospital'),\n",
       " ('g', 's', 'Campus de San Vicente'),\n",
       " ('g', 's', 'Department of Veterinary Medicine'),\n",
       " ('g', 's', 'Biosci Biotechnol Biochem'),\n",
       " ('s', 'g', 'Berkeley'),\n",
       " ('s', 'g', 'Mayo Clinic'),\n",
       " ('g', 's', 'Charlestown'),\n",
       " ('g', 's', 'Birmingham'),\n",
       " ('g', 's', 'Bar Harbor'),\n",
       " ('s', 'g', 'Cairo University'),\n",
       " ('g', 's', 'Baylor College of Medicine'),\n",
       " ('g', 's', 'Copenhagen'),\n",
       " ('g', 's', 'Bundeswehr Institute of Microbiology'),\n",
       " ('g', 's', 'Central Institute of the Bundeswehr Medical Service'),\n",
       " ('g', 's', 'As'),\n",
       " ('g', 's', 'Bethesda'),\n",
       " ('g', 's', 'Buenos Aires'),\n",
       " ('g', 's', 'College of Life Sciences'),\n",
       " ('g', 's', 'Aurora'),\n",
       " ('g', 's', 'New Haven'),\n",
       " ('g', 's', 'Argonne National Laboratory'),\n",
       " ('g', 's', 'O'),\n",
       " ('g', 's', 'Chicago'),\n",
       " ('g', 's', 'Norwich Research Park'),\n",
       " ('g', 's', 'Aarhus University'),\n",
       " ('g', 's', 'Belfast City Hospital'),\n",
       " ('g', 's', 'Baltimore'),\n",
       " ('g', 's', 'Central South University'),\n",
       " ('g', 's', 'Albert Einstein College of Medicine'),\n",
       " ('g', 's', 'Austria Queensland University of Technology'),\n",
       " ('g', 's', 'Chinese Academy of Sciences'),\n",
       " ('g', 's', 'Cornell University'),\n",
       " ('g', 's', 'Chapel Hill'),\n",
       " ('g', 's', 'Bronx'),\n",
       " ('g', 's', 'Center for Public Health Genomics'),\n",
       " ('s', 'g', 'Catalan Institute for Research and Advanced Studies')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## My main erors appear to be mis labeled as instituions --by me!-- and vice versa\n",
    "## and the classifier has correctly labeled them..\n",
    "## However, the classifier also likes to label cities as schools, despite they being a \n",
    "## single word...\n",
    "list(set(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## below is used to see what is marked as an institution\n",
    "sguess = []\n",
    "i = 0\n",
    "for (name, tag) in test_set:\n",
    "    guess = classifier.classify(name)\n",
    "    phrase2 = phrasesLookedUp[i]\n",
    "    if tag == 's':\n",
    "        sguess.append( (tag, guess, phrase2) )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## lets see how a decision tree does\n",
    "classifier_dt = nltk.classify.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696\n"
     ]
    }
   ],
   "source": [
    "## this error rate looks a bit better than for the bayes.\n",
    "print nltk.classify.accuracy(classifier_dt, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 's', 'Cardiff University School of Medicine'),\n",
       " ('g', 's', 'China National Rice Research Institute'),\n",
       " ('g', 's', 'College of Veterinary Medicine'),\n",
       " ('g', 's', 'Animal Nutrition and Health'),\n",
       " ('g', 's', 'Alfaisal University'),\n",
       " ('g', 's', 'Charite Medical University Berlin'),\n",
       " ('g', 's', 'Broad Institute of MIT and Harvard'),\n",
       " ('g', 's', 'China Shanghai Institute of Pharmaceutical Industry'),\n",
       " ('g', 's', 'Nottingham University Hospitals NHS Trust'),\n",
       " ('s', 'g', 'Denmark Max Planck Institute for Informatics'),\n",
       " ('g', 's', 'Danish Technical University'),\n",
       " ('g', 's', 'College of Basic Medicine'),\n",
       " ('g', 's', 'Anhui Agricultural University'),\n",
       " ('s', 'g', 'Berkeley'),\n",
       " ('s', 'g', 'Mayo Clinic'),\n",
       " ('g', 's', 'Beth Israel Deaconess Medical Center'),\n",
       " ('g', 's', 'National Institute of Neuroscience'),\n",
       " ('g', 's', 'Dalhousie University'),\n",
       " ('g', 's', 'Central South University'),\n",
       " ('g', 's', 'Brandeis University National Center for Behavioral Genomics'),\n",
       " ('g', 's', 'Baylor College of Medicine'),\n",
       " ('g', 's', 'Aarhus University'),\n",
       " ('g', 's', 'Bloomberg School of Public Health'),\n",
       " ('g', 's', 'Dalian Medical University'),\n",
       " ('g', 's', 'Departement Genomes et Genetique'),\n",
       " ('g', 's', 'Cornell University'),\n",
       " ('g', 's', 'Botanical Institute II'),\n",
       " ('s', 'g', 'Massachusetts General Hospital and Harvard Medical School')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I see a lot of reasonable corrections to my assignments, so the \n",
    "## classifier is doing a better job than I.  Plus I don't see\n",
    "## cities being named as institutions.  \n",
    "\n",
    "errors = []\n",
    "i = 0\n",
    "for (name, tag) in test_set:\n",
    "    guess = classifier.classify(name)\n",
    "    phrase3 = phrasesLookedUp[i]\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, phrase3) )\n",
    "    i += 1\n",
    "\n",
    "list(set(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## At this point, I think the decision tree is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitIntoPhrases(affiliation):\n",
    "    '''this long function is need to take each affilation string, and break it down into all its \n",
    "    possible deliminated pieces, and then take those and convert it to a list of 'phrases', one or more \n",
    "    of these phrases being a poissible institution name'''\n",
    "    \n",
    "    splitcomma = []\n",
    "    splitperiod = []\n",
    "    splitsemicolon = []\n",
    "    splitspace = []\n",
    "    if len(affiliation) > 1:\n",
    "        #print (x)\n",
    "        s = affiliation[1]\n",
    "        s = s[:-1]\n",
    "        \n",
    "        t = s.split(\",\")\n",
    "        if len(t)>2:\n",
    "            t = t[:-2]  # remove journal names\n",
    "        else:\n",
    "            t = \" \"\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitcomma.append(t)\n",
    "        t = s.split(\".\")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitperiod.append(t)\n",
    "        t = s.split(\";\")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitsemicolon.append(t)\n",
    "        t = s.split(\" \")\n",
    "        for x in range(0, len(t)):\n",
    "            if isinstance(t[x], str):\n",
    "                t[x] = str(t[x]).lstrip()\n",
    "                t[x] = str(t[x]).rstrip()\n",
    "        splitspace.append(t)\n",
    "    else:\n",
    "        splitcomma.append(\"\")\n",
    "        splitperiod.append(\"\")\n",
    "        splitsemicolon.append(\"\")\n",
    "        splitspace.append(\"\")  ## not used\n",
    "    listterms = []\n",
    "    \n",
    "    for x in range(0, len(splitcomma)):\n",
    "        temp = []\n",
    "        t = []\n",
    "        for y in splitcomma[x]:\n",
    "            if not stringHasPunct(y):\n",
    "                t.append(y)\n",
    "        temp.append(t)\n",
    "        t = []\n",
    "        for y in splitsemicolon[x]:\n",
    "            if not stringHasPunct(y):\n",
    "                t.append(y)\n",
    "        temp.append(t)\n",
    "        t = []\n",
    "        for y in splitperiod[x]:\n",
    "            if not stringHasPunct(y):\n",
    "                t.append(y)\n",
    "        temp.append(t)\n",
    "        listterms.append(temp)\n",
    "    return listterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I might as well train now on the full set of labeled data.\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets)\n",
    "classifier_dt = nltk.classify.DecisionTreeClassifier.train(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "predictions = []\n",
    "for x in pubs:\n",
    "    listphrases = splitIntoPhrases(x)\n",
    "    t = []\n",
    "    for i in listphrases:\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                p = classifier_dt.classify(document_features(x, k, word_feature))\n",
    "                if p == 's':  \n",
    "                    t.append(k)\n",
    "                else:\n",
    "                    t.append('NA')\n",
    "    predictions.append(t)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "affiliations = []\n",
    "for x in predictions:\n",
    "    affiliations.append(list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in affiliations:\n",
    "    if 'NA' in x:\n",
    "        x.remove('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Shanghai Jiao Tong University School of Medicine',\n",
       "  'University of California at San Diego'],\n",
       " [],\n",
       " [],\n",
       " ['University of Pennsylvania Perelman School of Medicine',\n",
       "  'Michigan State University',\n",
       "  'Kawasaki Medical School',\n",
       "  'Graduate School of Medicine',\n",
       "  'Kyoto University'],\n",
       " [],\n",
       " ['Creighton University'],\n",
       " ['Wageningen University', 'Faculty of Medicine University of Helsinki'],\n",
       " [],\n",
       " ['Tamagawa University', 'The University of Tokyo'],\n",
       " ['University of Turku and Abo Academy']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affiliations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g1 = []\n",
    "for x in affiliations:\n",
    "    if len(x)>1:\n",
    "        for i in range(0, len(x)):\n",
    "            for j in range(i+1, len(x)):\n",
    "                g1.append([x[i], x[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now save the undirected graph nodes\n",
    "f = open('IS620Project3.csv','w')\n",
    "for x in g1:\n",
    "    f.write(str(x[0]) + \",\" +  str(x[1]) + '\\n') \n",
    "f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
